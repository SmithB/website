{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import hvplot.xarray\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "# N.B.  This notebook is a lot more interesting if initialized with \n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-fighter",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "ICESat-2 is a laser altimeter designed to precisely measure the height of snow and ice surfaces using green lasers with small footprints.  Although ICESat-2 doesn't measure surface heights with the same spatial density as airborne laser altimeters, its global spatial coverage makes it a tempting source of free data about snow surfaces.  In this tutorial we will:\n",
    "\n",
    "1. Give a brief overview of ICESat-2\n",
    "\n",
    "2. Show how to find ICESat-2 granues using the IcePyx metadata search tool\n",
    "\n",
    "3. Download some ATL03 photon data from the openAltimetry web service\n",
    "\n",
    "4. Request custom processed height estimates from the SlideRule project.\n",
    "\n",
    "## ICESat-2 measurements and coverage\n",
    "\n",
    "ICESat-2 measures surface heights with three laser beams, grouped into six pairs separated by 3 km, with a 90-m separation between the beams in each pair.\n",
    "\n",
    "Here's a sketch of how this looks (image credit: NSIDC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('https://nsidc.org/sites/nsidc.org/files/images/atlas-beam-pattern.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-panel",
   "metadata": {},
   "source": [
    "ICESat-2 flies a repeat orbit with 1387 ground tracks every 91 days, but over Grand Mesa, the collection strategy (up until now) has designed to optimize spatial coverage, so the measurements are shifted to the left and right of the repeat tracks to help densify the dataset.  We should expect to see tracks running (approximately) north-south over the Mesa, in tripplets of pairs that are scattered from east to west.   Because clouds often block the laser, not every track will return usable data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-victory",
   "metadata": {},
   "source": [
    "## Basemap (Sentinel)\n",
    "\n",
    "To get a sense of where the data are, we're going to use an Sentinel SAR image of Grand Mesa.  I've stolen this snippet of code from the SAR tutorial for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDAL environment variables to efficiently read remote data\n",
    "os.environ['GDAL_DISABLE_READDIR_ON_OPEN']='EMPTY_DIR' \n",
    "os.environ['AWS_NO_SIGN_REQUEST']='YES' \n",
    "\n",
    "# Data is stored in a public S3 Bucket\n",
    "url = 's3://sentinel-s1-rtc-indigo/tiles/RTC/1/IW/12/S/YJ/2016/S1B_20161121_12SYJ_ASC/Gamma0_VV.tif'\n",
    "\n",
    "# These Cloud-Optimized-Geotiff (COG) files have 'overviews', low-resolution copies for quick visualization\n",
    "XR=[725000.0, 767000.0]\n",
    "YR=[4.30e6, 4.34e6]\n",
    "# open the dataset\n",
    "da = rioxarray.open_rasterio(url, overview_level=1).squeeze('band')#.clip_box([712410.0, 4295090.0, 797010.0, 4344370.0])\n",
    "da=da.where((da.x>XR[0]) & (da.x < XR[1]), drop=True)\n",
    "da=da.where((da.y>YR[0]) & (da.y < YR[1]), drop=True)\n",
    "dx=da.x[1]-da.x[0]\n",
    "SAR_extent=[da.x[0]-dx/2, da.x[-1]+dx/2, np.min(da.y)-dx/2, np.max(da.y)+dx/2]\n",
    "\n",
    "# Prepare coordinate transformations into the basemap coordinate system\n",
    "from pyproj import Transformer, CRS\n",
    "crs=CRS.from_wkt(da['spatial_ref'].spatial_ref.crs_wkt)\n",
    "to_image_crs=Transformer.from_crs(crs.geodetic_crs, crs)\n",
    "to_geo_crs=Transformer.from_crs(crs, crs.geodetic_crs)\n",
    "\n",
    "corners_lon, corners_lat=to_geo_crs.transform(np.array(XR)[[0, 1, 1, 0, 0]], np.array(YR)[[0, 0, 1, 1, 0]])\n",
    "lonlims=[np.min(corners_lat), np.max(corners_lat)]\n",
    "latlims=[np.min(corners_lon), np.max(corners_lon)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-bicycle",
   "metadata": {},
   "source": [
    "## Searching for data using IcePyx\n",
    "\n",
    "The IcePyx library has functions for searching for ICEsat-2 data, as well as subsetting it and retrieving it from NSIDC.  We're going to use the search functions today, because we don't need to retrieve the complete ICESat-2 products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import icepyx as ipx\n",
    "\n",
    "region_a = ipx.Query('ATL03', [lonlims[0], latlims[0], lonlims[1], latlims[1]], ['2018-12-01','2021-06-01'], \\\n",
    "                          start_time='00:00:00', end_time='23:59:59')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-qatar",
   "metadata": {},
   "source": [
    "To run this next section, you'll need to setup your netrc file to connect to nasa earthdata.  During the hackweek we will use machine credentials, but afterwards, you may need to use your own credentials.  The login procedure is in the next cell, commented out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-beatles",
   "metadata": {},
   "source": [
    "#earthdata_uid = 'your_name_here'\n",
    "#email = 'your@email'\n",
    "#region_a.earthdata_login(earthdata_uid, email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-calculator",
   "metadata": {},
   "source": [
    "Once we're logged in, the avail_granules() fetches a list of available ATL03 granules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_a.avail_granules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-reserve",
   "metadata": {},
   "source": [
    "The filename for each granule (which contains lots of handy information) is in the 'producer_granule_id' field: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_a.granules.avail[0]['producer_granule_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-oracle",
   "metadata": {},
   "source": [
    "The filename contains ATL03_YYYYMMDDHHMMSS_TTTTCCRR_rrr_vv.h5 where:\n",
    "\n",
    " * YYYMMDDHHMMSS gives the date (to the second) of the start of the granule\n",
    " * TTTT gives the ground-track number\n",
    " * CC gives the cycle number \n",
    " * RR gives the region (what part of the orbit this is) \n",
    " * rrr_vv give the release and version\n",
    " \n",
    " Let's strip out the date using a regular expression, and see when ICESat-2 flew over Grand Mesa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATLAS_re=re.compile('ATL.._(?P<year>\\d\\d\\d\\d)(?P<month>\\d\\d)(?P<day>\\d\\d)\\d+_(?P<track>\\d\\d\\d\\d)')\n",
    "\n",
    "date_track=[]\n",
    "for count, item in enumerate(region_a.granules.avail):\n",
    "    granule_info=ATLAS_re.search(item['producer_granule_id']).groupdict()\n",
    "    date_track += [ ('-'.join([granule_info[key] for key in ['year', 'month', 'day']]), granule_info['track'])]\n",
    "\n",
    "# print the first ten dates and ground tracks, plus their indexes\n",
    "[(count, dt) for count, dt in enumerate(date_track[0:10])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-cause",
   "metadata": {},
   "source": [
    "From this point, the very capable icepyx interface allows you to order either full data granules or subsets of granules from NSIDC.  Further details are available from https://icepyx.readthedocs.io/en/latest/, and their 'examples' pages are quite helpful.  Note that ATL03 photon data granules are somewhat cumbersome, so downloading them without subsetting will be time consuming, and requesting subsetting from NSIDC may take a while.  \n",
    "\n",
    "## Ordering photon data from openAltimetry\n",
    "For ordering small numbers of points (up to one degree worth of data), the openAltimetry service provides very quick and efficient access to a simplified version of the ATL03 data.  Their API (https://openaltimetry.org/data/swagger-ui/) allows us to build web queries for the data.  We'll use that for a quick look at the data over Grand Mesa, initially reading just one central beam pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OA(date_track, lonlims, latlims, beamnames=[\"gt1l\",\"gt1r\",\"gt2l\",\"gt2r\",\"gt3l\",\"gt3r\"]):\n",
    "    '''\n",
    "    retrieve ICESat2 ATL03 data from openAltimetry\n",
    "    \n",
    "    Inputs:\n",
    "        date_track: a list of tuples.  Each contains a date string \"YYYY-MM-DD\" and track number (4-character string)\n",
    "        lonlims: longitude limits for the search\n",
    "        latlims: latitude limits for the search\n",
    "        beamnames: list of strings for the beams\n",
    "    outputs:\n",
    "        a dict containing ATL03 data by beam name\n",
    "    \n",
    "    Due credit:\n",
    "        Much of this code was borrowed Philipp Arndt's Pond Picker repo: https://github.com/fliphilipp/pondpicking\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    IS2_data={}\n",
    "    for this_dt in date_track:\n",
    "        this_IS2_data={}\n",
    "        for beamname in beamnames:\n",
    "            oa_url = 'https://openaltimetry.org/data/api/icesat2/atl03?minx={minx}&miny={miny}&maxx={maxx}&maxy={maxy}&trackId={trackid}&beamName={beamname}&outputFormat=json&date={date}&client=jupyter'\n",
    "            oa_url = oa_url.format(minx=lonlims[0],miny=latlims[0],maxx=lonlims[1], maxy=latlims[1], \n",
    "                                   trackid=this_dt[1], beamname=beamname, date=this_dt[0], sampling='true')\n",
    "            #.conf_ph = ['Noise','Buffer', 'Low', 'Medium', 'High']\n",
    "            if True:\n",
    "                r = requests.get(oa_url)\n",
    "                data = r.json()\n",
    "                D={}\n",
    "                D['lat_ph'] = []\n",
    "                D['lon_ph'] = []\n",
    "                D['h_ph'] = []\n",
    "                D['conf_ph']=[]\n",
    "                conf_ph = {'Noise':0, 'Buffer':1, 'Low':2, 'Medium':3, 'High':4}\n",
    "                for beam in data:\n",
    "                    for photons in beam['series']:\n",
    "                        for conf, conf_num in conf_ph.items():         \n",
    "                            if conf in photons['name']:\n",
    "                                for p in photons['data']:\n",
    "                                    \n",
    "                                    D['lat_ph'].append(p[0])\n",
    "                                    D['lon_ph'].append(p[1])\n",
    "                                    D['h_ph'].append(p[2])\n",
    "                                    D['conf_ph'].append(conf_num)\n",
    "                    D['x_ph'], D['y_ph']=to_image_crs.transform(D['lat_ph'], D['lon_ph'])\n",
    "                for key in D:\n",
    "                    D[key]=np.array(D[key])\n",
    "                if len(D['lat_ph']) > 0:\n",
    "                    this_IS2_data[beamname]=D\n",
    "            #except Exception as e:\n",
    "            #    print(e)\n",
    "            #    pass\n",
    "        if len(this_IS2_data.keys()) > 0:\n",
    "            IS2_data[this_dt] = this_IS2_data\n",
    "    return IS2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submitting all of these requests should take about 1 minute\n",
    "IS2_data=get_OA(date_track, lonlims, latlims, ['gt2l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(np.array(da)[::-1,:], origin='lower', extent=SAR_extent, cmap='gray', clim=[0, 0.5])#plt.figure();\n",
    "\n",
    "for dt, day_data in IS2_data.items():\n",
    "    for beam, D in day_data.items():\n",
    "        plt.plot(D['x_ph'][::10], D['y_ph'][::10], '.', markersize=3, label=str(dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-stock",
   "metadata": {},
   "source": [
    "What we see in this plot is Grand Mesa, with lines showing data from the center beams of several tracks passing across it.  A few of these tracks have been repeated, but most are offset from the others.  Looking at these, it should be clear that the quality of the data is not consistent from track to track.  Some are nearly continuous, others have gaps, and other still have no data at all and are not plotted here.  Remember, though, that what we've plotted here are just the center beams.  There are a total of two more beam pairs, and a total of five more beams!\n",
    "\n",
    "To get an idea of what the data look like, we'll pick one of the tracks and plot its elevation profile.  In interactive mode (%matplotlib widget) it's possible to zoom in on the plot, query the x and y limits, and use these to identify the data for the track that intersects an area of interest.  I've done this to pick two good-looking tracks, but you can uncomment the first two lines here and zoom in yourself to look at other tracks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-percentage",
   "metadata": {},
   "outputs": [],
   "source": [
    "XR=plt.gca().get_xlim()\n",
    "YR=plt.gca().get_ylim()\n",
    "print(XR)\n",
    "print(YR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XR=plt.gca().get_xlim()\n",
    "#YR=plt.gca().get_ylim()\n",
    "XR=(740773.7483556366, 741177.9430390946)\n",
    "YR=(4325197.508090873, 4325728.013612912)\n",
    "\n",
    "dts_in_axes=[]\n",
    "for dt, day_data in IS2_data.items():\n",
    "    for beam, D in day_data.items():\n",
    "        if np.any(\n",
    "            (D['x_ph'] > XR[0]) & (D['x_ph'] < XR[1]) &\n",
    "            (D['y_ph'] > np.min(YR)) & (D['y_ph'] < np.max(YR))):\n",
    "            dts_in_axes += [dt]\n",
    "dts_in_axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-arizona",
   "metadata": {},
   "source": [
    "Now we can get the full (six-beam) dataset for one of these tracks and plot it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "dts_in_axes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_track_data=get_OA([dts_in_axes[0]], lonlims, latlims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig=plt.figure(); \n",
    "hax=fig.subplots(1, 2)\n",
    "plt.sca(hax[0])\n",
    "plt.imshow(np.array(da)[::-1,:], origin='lower', extent=SAR_extent, cmap='gray', clim=[0, 0.5])#plt.figure();\n",
    "\n",
    "for dt, day_data in full_track_data.items():\n",
    "    for beam, D in day_data.items():\n",
    "        plt.plot(D['x_ph'], D['y_ph'],'.', markersize=1)\n",
    "plt.title(dts_in_axes[0])\n",
    "\n",
    "plt.sca(hax[1])\n",
    "D=day_data['gt2l']\n",
    "colors_key={((0,1)):'k', (2,3,4):'r'}\n",
    "for confs, color in colors_key.items():\n",
    "    for conf in confs:\n",
    "        these=np.flatnonzero(D['conf_ph']==conf)\n",
    "        plt.plot(D['y_ph'][these], D['h_ph'][these],'.', color=color, markersize=1)#label=','.join(list(confs)))\n",
    "plt.ylabel('WGS-84 height, m');\n",
    "plt.xlabel('UTM-12 northing, m');\n",
    "plt.title('gt2l');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-extra",
   "metadata": {},
   "source": [
    "On the left we see a plot of all six beams crossing (or almost crossing) Grand Mesa, in April of 2020.  If you zoom in on the plot, you can distinguish the beam pairs into separate beams.  On the right, we see one of the central beams crossing the mesa from south to north.  There is a broad band of noise photons that were close enough to the ground to be telemetered by the satellite, and a much narrower band (in red) of photons identified by the processing software as likely coming from the ground."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-columbus",
   "metadata": {},
   "source": [
    "These data give a maximum of detail about what the surface looks like to ICESat-2.  to reduce this to elevation data, telling the surface height at specific locations, there are a few options:\n",
    "    \n",
    "    1. Download higher-level products (i.e. ATL06, ATL08) from NSIDC\n",
    "    2. Calculate statistics of the photons (i.e. a running mean of the flagged photon heights\n",
    "    3. Ask the SlideRule service to calculate along-track averages of the photon heights.\n",
    "\n",
    "We're going to try (3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-aquatic",
   "metadata": {},
   "source": [
    "## Ordering surface-height segments from SlideRule\n",
    "\n",
    "SildeRule is a new and exciting (to me) system that does real-time processing of ICESat-2 data _in the cloud_ while also offering efficient web-based delivery of data products.  It's new, and it's not available for all locations, but Grand Mesa is one of the test sites, so we should be able to get access to the full set of ATL03 data there.\n",
    "[MORE WORK TO GO HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-volunteer",
   "metadata": {},
   "source": [
    "You'll need to install the sliderule-python package, available from https://github.com/ICESat2-SlideRule/sliderule-python\n",
    "This package has been installed on the hub, but if you need it, these commands will install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! [ -d sliderule-python ] || git clone https://github.com/ICESat2-SlideRule/sliderule-python.git \n",
    "#! cd sliderule-python; python setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-marks",
   "metadata": {},
   "source": [
    "We will submit a query to sliderule to process all of the data that CMR finds for our region, fitting 20-meter line-segments to all of the photons with medium-or-better signal confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sliderule import icesat2\n",
    "\n",
    "# initialize\n",
    "icesat2.init(\"icesat2sliderule.org\", verbose=False)\n",
    "\n",
    "# region of interest polygon\n",
    "region = [ {\"lon\":lon_i, \"lat\":lat_i} for lon_i, lat_i in \n",
    "          zip(np.array(lonlims)[[0, -1, -1, 0, 0]],  np.array(latlims)[[0, 0, -1, -1, 0]])]\n",
    "\n",
    "# request parameters\n",
    "parms = {\n",
    "    \"poly\": region,  # request the polygon defined by our lat-lon bounds\n",
    "    \"srt\": icesat2.SRT_LAND, # request classification based on the land algorithm\n",
    "    \"cnf\": icesat2.CNF_SURFACE_MEDIUM, # use all photons of low confidence or better\n",
    "    \"len\": 20.0,  # fit data in overlapping 40-meter segments\n",
    "    \"res\": 10.0,  # report one height every 20 m\n",
    "    \"ats\":5., #report a segment only if it contains at least 2 photons separated by 5 m\n",
    "    \"maxi\": 6,  # allow up to six iterations in fitting each segment to the data\n",
    "}\n",
    "\n",
    "# make request\n",
    "rsps = icesat2.atl06p(parms, \"atlas-s3\")\n",
    "\n",
    "# save the result in a dataframe\n",
    "df = pd.DataFrame(rsps)\n",
    "\n",
    "# calculate the polar-stereographic coordinates:\n",
    "df['x'], df['y']=to_image_crs.transform(df['lat'], df['lon'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-tiger",
   "metadata": {},
   "source": [
    "Let's find all the segments from rgt 295, cycle 7 and map their heights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-adelaide",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); \n",
    "plt.imshow(np.array(da)[::-1,:], origin='lower', extent=SAR_extent, cmap='gray', clim=[0, 0.5])#plt.figure();\n",
    "ii=(df['rgt']==295) & (df['cycle']==7)\n",
    "plt.scatter(df['x'][ii], df['y'][ii],4, c=df['h_mean'][ii], cmap='gist_earth')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-emphasis",
   "metadata": {},
   "source": [
    "As it turns out, for track 295 cycles 7 and 8 are nearly exact repeats.  Cycle 7 was April 2020, cycle 8 was July 2020.  Could it be that we can measure snow depth in April by comparing the two?  Let's plot spot 3 for both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure();\n",
    "ii=(df['rgt']==295) & (df['cycle']==7) & (df['spot']==3)\n",
    "plt.plot(df['y'][ii], df['h_mean'][ii],'.', label='April')\n",
    "ii=(df['rgt']==295) & (df['cycle']==8) & (df['spot']==3)\n",
    "plt.plot(df['y'][ii], df['h_mean'][ii],'.', label='July')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('polar stereographic northing, m')\n",
    "plt.ylabel('height, m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-commerce",
   "metadata": {},
   "source": [
    "This looks like July is lower than April, but the tracks are not an exact repeat, so there's room for more work here.  This could be a good project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-completion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
